import os
import torch
import json
import re
from PIL import Image
import warnings
warnings.filterwarnings("ignore")
torch.set_warn_always(False)

# ç¦»çº¿ç¯å¢ƒé…ç½®ï¼ˆå±è”½æ‰€æœ‰æ—¥å¿—ï¼Œä¸“æ³¨æ¨ç†ï¼‰
os.environ["HF_HUB_OFFLINE"] = "1"
os.environ["LOCAL_FILES_ONLY"] = "1"
os.environ["TORCH_NO_WARNINGS"] = "1"
os.environ["TRANSFORMERS_TRUST_REMOTE_CODE"] = "1"
os.environ["TRANSFORMERS_VERBOSITY"] = "critical"

# å›ºå®šè·¯å¾„ï¼ˆç¡®ä¿æ­£ç¡®ï¼Œæ— éœ€ä¿®æ”¹ï¼‰
MODEL_PATH = "/root/autodl-tmp/models/qwen2-vl-local"
IMG_DIR = "/root/autodl-tmp/datasets/mathv_3040/images"
ANN_PATH = "/root/autodl-tmp/datasets/mathv_3040/annotations.json"

# æ ¸å¿ƒé…ç½®ï¼ˆæç®€ï¼Œå®Œå…¨é€‚é…Qwen2-VLï¼‰
DEVICE = torch.device("cuda:0") if torch.cuda.is_available() else "cpu"
GEN_MAX_LEN = 2  # é€‚é…æ•°å­—/å­—æ¯GTï¼Œè¶³å¤Ÿä¸”ä¸å†—ä½™
PROMPT = "ç›´æ¥å›ç­”ï¼Œä»…è¾“å‡ºæ•°å­—æˆ–å¤§å†™å­—æ¯ï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚"

# ç­”æ¡ˆåå¤„ç†ï¼ˆæç®€é«˜æ•ˆï¼Œæ— å…œåº•ï¼Œä¿ç•™æ¨¡å‹çœŸå®è¾“å‡ºï¼‰
def clean_answer(s):
    if not s or s.strip() == "":
        return ""
    s = str(s).strip().upper()
    res = re.findall(r'[0-9A-Z]+', s)  # æå–æ•°å­—/å­—æ¯ï¼ˆå•/å¤šå­—ç¬¦å‡é€‚é…ï¼‰
    return res[0] if res else ""

# åŠ è½½æ•°æ®é›†ï¼ˆå‰50æ¡æµ‹è¯•ï¼Œä¸€é”®åˆ‡å…¨é‡ï¼Œå¼ºåŒ–æ ¡éªŒï¼‰
def load_dataset():
    dataset = []
    try:
        with open(ANN_PATH, "r", encoding="utf-8") as f:
            anns = json.load(f)[:50]  # æµ‹è¯•å‰50æ¡ï¼Œå¿«å‡ºç»“æœ
            # anns = json.load(f)  # å…¨é‡3040æ¡ï¼Œæµ‹è¯•æˆåŠŸåæ‰“å¼€è¿™è¡Œ
    except Exception as e:
        print(f"âŒ æ ‡æ³¨æ–‡ä»¶é”™è¯¯ï¼š{str(e)[:30]}")
        return []
    for idx, a in enumerate(anns, 1):
        img_path = os.path.join(IMG_DIR, a["image_name"])
        if os.path.exists(img_path) and img_path.lower().endswith(('.jpg','.jpeg','.png')):
            dataset.append({
                "img_path": img_path,
                "question": a["question"],
                "gt": a["gt"] if "gt" in a else a["answer"],
                "idx": idx
            })
    print(f"âœ… åŠ è½½ {len(dataset)} æ¡æœ‰æ•ˆæ ·æœ¬ | è®¾å¤‡ï¼š{DEVICE}")
    return dataset

# åŠ è½½æ¨¡å‹+å¤„ç†å™¨ï¼ˆ4.57.6ä¸“å±ï¼Œæ— ä»»ä½•å†—ä½™é…ç½®ï¼‰
def load_model():
    from transformers import Qwen2VLForConditionalGeneration, Qwen2VLProcessor
    # åŠ è½½ä¸“å±å¤„ç†å™¨ï¼ˆåŸç”Ÿé…ç½®ï¼Œæ— å†²çªï¼‰
    processor = Qwen2VLProcessor.from_pretrained(
        MODEL_PATH, local_files_only=True, trust_remote_code=True, use_fast=False
    )
    processor.tokenizer.pad_token = processor.tokenizer.eos_token
    print(f"âœ… Qwen2-VLå¤„ç†å™¨åŠ è½½å®Œæˆ")
    # åŠ è½½ä¸“å±æ¨¡å‹ï¼ˆæç®€é…ç½®ï¼Œé€‚é…RTX4090ï¼‰
    model = Qwen2VLForConditionalGeneration.from_pretrained(
        MODEL_PATH,
        local_files_only=True,
        dtype=torch.float16,
        device_map={"": 0},
        trust_remote_code=True,
        low_cpu_mem_usage=True,
        attn_implementation="eager"
    ).eval()
    print(f"âœ… Qwen2-VLæ¨¡å‹åŠ è½½å®Œæˆ | 4.57.6 æ— å†²çª")
    return model, processor

# æ ¸å¿ƒæ¨ç†ï¼ˆå½»åº•ç§»é™¤æ‰€æœ‰å†—ä½™å‚æ•°ï¼ŒåŸç”Ÿçº¯æ¨ç†ï¼Œæ— ä»»ä½•é”™è¯¯ï¼ï¼‰
def infer(model, processor, sample):
    try:
        # 1. å®‰å…¨åŠ è½½å›¾åƒ
        with Image.open(sample["img_path"]) as f:
            image = f.convert("RGB")
        # 2. æ„é€ åŸç”Ÿè¾“å…¥ï¼ˆå¤„ç†å™¨è‡ªåŠ¨ç”Ÿæˆæ‰€æœ‰å¿…éœ€å‚æ•°ï¼Œæ— æ‰‹åŠ¨å¹²é¢„ï¼‰
        prompt = f"{sample['question']} {PROMPT}"
        inputs = processor(images=image, text=prompt, return_tensors="pt").to(DEVICE, torch.float16)
        # 3. æ¨¡å‹çº¯æ¨ç†ï¼ˆä»…ä¿ç•™æ ¸å¿ƒæœ‰æ•ˆå‚æ•°ï¼Œå½»åº•ç§»é™¤æ‰€æœ‰å†—ä½™ï¼ï¼‰
        with torch.no_grad(), torch.cuda.amp.autocast(enabled=DEVICE.type=="cuda"):
            generate_ids = model.generate(
                **inputs,
                max_new_tokens=GEN_MAX_LEN,  # ä»…ç”ŸæˆæŒ‡å®šé•¿åº¦
                do_sample=False,  # è´ªå¿ƒæœç´¢ï¼Œæ— éšæœº
                eos_token_id=processor.tokenizer.eos_token_id,
                pad_token_id=processor.tokenizer.pad_token_id
            )
        # 4. è§£ç çœŸå®è¾“å‡ºï¼ˆä»…æå–æ¨¡å‹ç”Ÿæˆéƒ¨åˆ†ï¼Œæ— å…œåº•ï¼‰
        gen_ids = generate_ids[:, inputs["input_ids"].shape[1]:]
        raw = processor.decode(gen_ids[0], skip_special_tokens=True).strip()
        pred = clean_answer(raw)
        return pred, raw
    except Exception as e:
        err = str(e)[:30].replace("\n","").replace(" ","")
        return "", f"err:{err}"

# ä¸»å‡½æ•°ï¼ˆå½©è‰²æ‰“å°ï¼Œç»Ÿè®¡çœŸå®å‡†ç¡®ç‡ï¼Œæ— ä»»ä½•äººå·¥å¹²é¢„ï¼‰
if __name__ == "__main__":
    print("="*90)
    print("ğŸ”´ Qwen2-VL æœ€ç»ˆçº¯æ¨ç†ç‰ˆ | 4.57.6 | é›¶å†—ä½™ | æ— å…œåº•/ä¸æŠ„GT")
    print("="*90)
    torch.cuda.empty_cache()
    # åŠ è½½æ•°æ®å’Œæ¨¡å‹
    data = load_dataset()
    if not data: exit()
    try:
        model, processor = load_model()
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥ï¼š{str(e)[:50]}")
        exit()
    # æ‰¹é‡æ¨ç†
    print("\nğŸš€ å¼€å§‹çº¯æ¨ç†...ï¼ˆæ— å…œåº•ï¼Œæ¨¡å‹è‡ªä¸»è¾“å‡ºï¼Œæœ‰å¯¹æœ‰é”™ï¼‰")
    total, correct = len(data), 0
    show_num = 20  # æ‰“å°å‰20æ¡ç»“æœ
    for s in data:
        pred, raw = infer(model, processor, s)
        if pred and pred == s["gt"]:
            correct += 1
        # å½©è‰²æ‰“å°
        if s["idx"] <= show_num:
            if pred and pred == s["gt"]:
                print(f"\033[32mæ ·æœ¬{s['idx']:2d} | GT:{s['gt']:3s} | PRED:{pred:3s} | RAW:{raw[:8]} âœ…\033[0m")
            else:
                print(f"\033[31mæ ·æœ¬{s['idx']:2d} | GT:{s['gt']:3s} | PRED:{pred:3s} | RAW:{raw[:15]} âŒ\033[0m")
    # æœ€ç»ˆç»Ÿè®¡
    acc = (correct/total)*100 if total>0 else 0.0
    torch.cuda.empty_cache()
    print("="*90)
    print(f"\033[34mğŸ”´ æ¨ç†å®Œæˆ | æ€»{total}æ¡ | æ­£ç¡®{correct}æ¡ | çœŸå®å‡†ç¡®ç‡ï¼š{acc:.1f}%\033[0m")
    print("="*90)
    print("ğŸ’¡ åˆ‡å…¨é‡ï¼šå°†load_datasetä¸­ anns = json.load(f)[:50] æ”¹ä¸º anns = json.load(f)")
    print("ğŸ’¡ ç»“æœè¯´æ˜ï¼šå‡†ç¡®ç‡é100%ä¸ºæ¨¡å‹çœŸå®èƒ½åŠ›ï¼Œæ— ä»»ä½•äººå·¥å…œåº•/æŠ„GTï¼")