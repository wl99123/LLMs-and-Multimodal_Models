from transformers import AutoTokenizer, Qwen2VLForConditionalGeneration, Qwen2VLProcessor
from PIL import Image
import json
import os
import torch
from sklearn.metrics import accuracy_score, f1_score
import warnings
warnings.filterwarnings("ignore")

# å…¨å±€ç¯å¢ƒé…ç½®ï¼ˆå±è”½å†—ä½™è­¦å‘Šï¼Œå¼ºåˆ¶å•å¡ï¼‰
os.environ["TRANSFORMERS_TRUST_REMOTE_CODE"] = "True"
os.environ["CUDA_VISIBLE_DEVICES"] = "0"
os.environ["TOKENIZERS_PARALLELISM"] = "false"
torch.set_grad_enabled(False)

# === å…¨å±€é…ç½®ï¼ˆå®Œå…¨åŒ¹é…ä½ çš„ç¯å¢ƒï¼ŒQwen2-VLå®˜æ–¹å‚æ•°ï¼‰ ===
model_path = "/root/autodl-tmp/models/qwen2-vl-local"
dataset_json_path = "/root/autodl-tmp/datasets/coco_vqa_1000/val_sample_1000.json"
dataset_img_dir = "/root/autodl-tmp/datasets/coco_vqa_1000/images"
DEVICE = "cuda:0" if torch.cuda.is_available() else "cpu"
MAX_NEW_TOKENS = 4  # Qwen2-VLçŸ­ç­”æ¡ˆè¶³å¤Ÿï¼ˆæ˜¯/å¦/æ•°å­—ï¼‰ï¼Œå‡å°‘å†—ä½™
TEST_NUM = 20  # æµ‹è¯•å‰20ä¸ªæ ·æœ¬
DEBUG = True  # æ‰“å°æœ‰æ•ˆæ ·æœ¬è¯¦æƒ…

# === è·¯å¾„å¼ºåˆ¶æ ¡éªŒï¼ˆå…³é”®è·¯å¾„ä¸å­˜åœ¨ç›´æ¥é€€å‡ºï¼‰ ===
assert os.path.exists(dataset_json_path), f"âŒ æ•°æ®é›†JSONä¸å­˜åœ¨: {dataset_json_path}"
assert os.path.isdir(model_path), f"âŒ æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_path}"
assert os.path.isdir(dataset_img_dir), f"âŒ å›¾åƒç›®å½•ä¸å­˜åœ¨: {dataset_img_dir}"
print(f"âœ… ç¯å¢ƒåˆå§‹åŒ–å®Œæˆ | è®¾å¤‡ï¼š{DEVICE} | æµ‹è¯•æ ·æœ¬æ•°ï¼š{TEST_NUM}")

# === æ ¸å¿ƒä¿®å¤1ï¼šåŠ è½½Qwen2-VLå®˜æ–¹ä¸“å±Processor+æ¨¡å‹ï¼ˆå¼ºåˆ¶é€‚é…æƒé‡å°ºå¯¸ï¼‰ ===
# åŠ è½½Qwen2-VLå®˜æ–¹å›¾æ–‡å¤„ç†å™¨ï¼ˆä¸€ç«™å¼å¤„ç†å›¾åƒ+æ–‡æœ¬ï¼Œå®˜æ–¹å”¯ä¸€æ¨èï¼‰
processor = Qwen2VLProcessor.from_pretrained(
    model_path,
    local_files_only=True,
    trust_remote_code=True
)
# åŠ è½½Qwen2-VLæ¨¡å‹ï¼ˆæ ¸å¿ƒï¼šignore_mismatched_sizes=True å¼ºåˆ¶é€‚é…æƒé‡å°ºå¯¸ï¼‰
model = Qwen2VLForConditionalGeneration.from_pretrained(
    model_path,
    local_files_only=True,
    trust_remote_code=True,
    torch_dtype=torch.float16 if DEVICE == "cuda:0" else torch.float32,
    device_map="auto",
    low_cpu_mem_usage=True,
    ignore_mismatched_sizes=True,  # æ ¸å¿ƒä¿®å¤ï¼šå¼ºåˆ¶å¿½ç•¥æƒé‡å°ºå¯¸ä¸åŒ¹é…
    attn_implementation="eager"   # å…¼å®¹ä½ç‰ˆæœ¬ï¼Œé¿å…flash attentionæŠ¥é”™
).to(DEVICE).eval()
# å¼ºåˆ¶è®¾ç½®ç‰¹æ®Štokenï¼ˆå¤„ç†å™¨å…œåº•åå†æ¬¡ç¡®è®¤ï¼‰
processor.tokenizer.pad_token = processor.tokenizer.eos_token
processor.tokenizer.unk_token = processor.tokenizer.pad_token
print(f"âœ… Qwen2-VLå®˜æ–¹æ¨¡å‹åŠ è½½æˆåŠŸ | è®¾å¤‡ï¼š{DEVICE}")
print(f"âœ… å·²å¼ºåˆ¶é€‚é…æƒé‡å°ºå¯¸ï¼Œå¿½ç•¥Conv3då½¢çŠ¶ä¸åŒ¹é…")

# === æ ¸å¿ƒå‡½æ•°ï¼šå®˜æ–¹Promptæ ¼å¼+æŒ‡æ ‡è®¡ç®—+å¹»è§‰æ£€æµ‹ï¼ˆæç®€å…œåº•ï¼‰ ===
def generate_vqa_prompt(question, use_cot=False):
    """Qwen2-VLå®˜æ–¹Promptæ ¼å¼ï¼ŒåŠ å…¥å›¾åƒæ ‡è¯†ï¼ˆå¿…é¡»åŠ ï¼‰"""
    question = str(question).strip() if question else "å›¾ç‰‡ä¸­å æ¯”æœ€å¤§çš„é¢œè‰²æ˜¯ä»€ä¹ˆï¼Ÿ"
    if use_cot:
        return f"æ ¹æ®å›¾ç‰‡å†…å®¹å›ç­”ä»¥ä¸‹é—®é¢˜ï¼Œä¸€æ­¥æ­¥æ¨ç†ï¼Œæœ€åä»…ç»™å‡ºç®€å•ç­”æ¡ˆï¼š{question}"
    else:
        return f"æ ¹æ®å›¾ç‰‡å†…å®¹å›ç­”ä»¥ä¸‹é—®é¢˜ï¼Œç›´æ¥ç»™å‡ºç®€å•ç­”æ¡ˆï¼Œä¸è¦å¤šä½™å†…å®¹ï¼š{question}"

def calculate_metrics(predictions, references):
    """ä¸¥æ ¼è¿‡æ»¤ç©ºå€¼ï¼Œè®¡ç®—å‡†ç¡®ç‡å’ŒåŠ æƒF1"""
    valid_pairs = [(p.strip(), r.strip()) for p, r in zip(predictions, references) if p and r]
    if not valid_pairs:
        return {"accuracy": 0.0, "f1": 0.0}
    preds, refs = zip(*valid_pairs)
    return {
        "accuracy": round(accuracy_score(refs, preds), 4),
        "f1": round(f1_score(refs, preds, average='weighted', zero_division=0), 4)
    }

def calculate_hallucination_rate(predictions, references):
    """è®¡ç®—å¹»è§‰ç‡ï¼šé¢„æµ‹ä¸çœŸå®ç­”æ¡ˆä¸ä¸€è‡´å³ä¸ºå¹»è§‰"""
    valid_count, hallucination_count = 0, 0
    for p, r in zip(predictions, references):
        p, r = p.strip(), r.strip()
        if p and r:
            valid_count += 1
            hallucination_count += 1 if p != r else 0
    return round(hallucination_count / valid_count if valid_count > 0 else 0.0, 4)

# === æ ¸å¿ƒä¿®å¤2ï¼šQwen2-VLå®˜æ–¹æ ‡å‡†å›¾æ–‡æ¨ç†ï¼ˆä»æ ¹æºè§£å†³Noneè¿­ä»£æŠ¥é”™ï¼‰ ===
def run_vqa_evaluation(use_cot=False):
    # åŠ è½½å¹¶é¢„å¤„ç†æ•°æ®é›†ï¼ˆä»…åšåŸºç¡€è¿‡æ»¤ï¼‰
    with open(dataset_json_path, 'r', encoding='utf-8') as f:
        raw_data = json.load(f)
    # ä»…ä¿ç•™æœ‰image_idã€questionã€answerçš„æœ‰æ•ˆæ ·æœ¬
    processed_data = [
        item for item in raw_data
        if isinstance(item, dict) and item.get("image_id") and item.get("question") and item.get("answer")
    ]
    if DEBUG and processed_data:
        print(f"\nğŸ” æ•°æ®é›†é¢„å¤„ç†å®Œæˆ | åŸå§‹æ ·æœ¬ï¼š{len(raw_data)} | æœ‰æ•ˆæ ·æœ¬ï¼š{len(processed_data)}")
        print(f"ğŸ” ç¬¬1ä¸ªæ ·æœ¬ç¤ºä¾‹ï¼š{processed_data[0]}")

    predictions, references = [], []
    valid_sample_count = 0

    print(f"\nğŸš€ å¼€å§‹VQAè¯„ä¼° | COTæ€ç»´é“¾ï¼š{'å¼€å¯' if use_cot else 'å…³é—­'}")
    for idx, item in enumerate(processed_data[:TEST_NUM]):
        try:
            # 1. æå–åŸºç¡€å­—æ®µï¼ˆæç®€å…œåº•ï¼‰
            img_id = str(item["image_id"]).strip()
            question = item["question"].strip()
            true_answer = item["answer"].strip()

            # 2. åŠ è½½å¹¶æ ¡éªŒå›¾åƒï¼ˆQwen2-VLå®˜æ–¹è¦æ±‚RGBæ ¼å¼ï¼‰
            img_name = f"COCO_val2014_{img_id.zfill(12)}.jpg"
            img_path = os.path.join(dataset_img_dir, img_name)
            if not os.path.exists(img_path):
                raise Exception(f"å›¾åƒä¸å­˜åœ¨ï¼š{img_name}")
            image = Image.open(img_path).convert("RGB")  # å®˜æ–¹å¼ºåˆ¶RGB

            # 3. æ ¸å¿ƒï¼šä½¿ç”¨å®˜æ–¹Processorä¸€ç«™å¼å¤„ç†å›¾åƒ+æ–‡æœ¬
            prompt = generate_vqa_prompt(question, use_cot)
            inputs = processor(
                images=image,
                text=prompt,
                return_tensors="pt",
                padding=True,
                truncation=True,
                max_length=512
            ).to(DEVICE, torch.float16 if DEVICE == "cuda:0" else torch.float32)

            # 4. Qwen2-VLå®˜æ–¹æ ‡å‡†æ¨ç†
            with torch.cuda.amp.autocast(enabled=DEVICE == "cuda:0"):
                outputs = model.generate(
                    **inputs,
                    max_new_tokens=MAX_NEW_TOKENS,
                    do_sample=False,
                    pad_token_id=processor.tokenizer.pad_token_id,
                    eos_token_id=processor.tokenizer.eos_token_id,
                    temperature=0.0,
                    num_beams=1,
                    use_cache=True
                )

            # 5. è§£æé¢„æµ‹ç»“æœ
            pred_answer = processor.decode(outputs[0], skip_special_tokens=True).strip()
            pred_answer = pred_answer.replace(prompt, "").strip() or "æ— "
            true_answer = true_answer or "æ— "

            # 6. ç»Ÿè®¡æœ‰æ•ˆæ ·æœ¬
            predictions.append(pred_answer)
            references.append(true_answer)
            valid_sample_count += 1

            # è°ƒè¯•æ‰“å°
            if DEBUG:
                print(f"âœ… æ ·æœ¬{idx}ï¼šæœ‰æ•ˆ | é—®é¢˜ï¼š{question[:30]} | çœŸå®ï¼š{true_answer} | é¢„æµ‹ï¼š{pred_answer}")

        except Exception as e:
            err_info = str(e)[:50].replace("\n", " ")
            print(f"âš ï¸  æ ·æœ¬{idx}ï¼šè·³è¿‡ | åŸå› ï¼š{err_info}")
            continue
        finally:
            if DEVICE == "cuda:0":
                torch.cuda.empty_cache()

    # è®¡ç®—æŒ‡æ ‡
    metrics = calculate_metrics(predictions, references)
    hallucination_rate = calculate_hallucination_rate(predictions, references)
    print(f"\nâœ… è¯„ä¼°å®Œæˆ | æµ‹è¯•æ ·æœ¬æ•°ï¼š{TEST_NUM} | æœ‰æ•ˆæ ·æœ¬æ•°ï¼š{valid_sample_count}")
    return metrics, hallucination_rate

# === ä¸»ç¨‹åºï¼šCOT/éCOTå¯¹æ¯”è¯„ä¼° ===
if __name__ == "__main__":
    # 1. ä¸å¯ç”¨COTè¯„ä¼°
    print("=" * 60)
    print("ğŸ“Š è¯„ä¼°æ¨¡å¼ï¼šä¸å¯ç”¨COTæ€ç»´é“¾ï¼ˆç›´æ¥å›ç­”ï¼‰")
    print("=" * 60)
    no_cot_metrics, no_cot_hallu = run_vqa_evaluation(use_cot=False)
    print(f"\nğŸ“ˆ ä¸å¯ç”¨COTè¯„ä¼°ç»“æœï¼š")
    print(f"å‡†ç¡®ç‡ï¼š{no_cot_metrics['accuracy']:.2%} | åŠ æƒF1ï¼š{no_cot_metrics['f1']:.4f} | å¹»è§‰ç‡ï¼š{no_cot_hallu:.2%}")

    # 2. å¯ç”¨COTè¯„ä¼°
    print("\n" + "=" * 60)
    print("ğŸ“Š è¯„ä¼°æ¨¡å¼ï¼šå¯ç”¨COTæ€ç»´é“¾ï¼ˆåˆ†æ­¥æ¨ç†ï¼‰")
    print("=" * 60)
    with_cot_metrics, with_cot_hallu = run_vqa_evaluation(use_cot=True)
    print(f"\nğŸ“ˆ å¯ç”¨COTè¯„ä¼°ç»“æœï¼š")
    print(f"å‡†ç¡®ç‡ï¼š{with_cot_metrics['accuracy']:.2%} | åŠ æƒF1ï¼š{with_cot_metrics['f1']:.4f} | å¹»è§‰ç‡ï¼š{with_cot_hallu:.2%}")

    # 3. å¯¹æ¯”æ€»ç»“
    print("\n" + "=" * 70)
    print("ğŸ“‹ COTæ€ç»´é“¾æ•ˆæœå¯¹æ¯”æ€»ç»“")
    print("=" * 70)
    acc_change = (with_cot_metrics['accuracy'] - no_cot_metrics['accuracy']) * 100
    f1_change = with_cot_metrics['f1'] - no_cot_metrics['f1']
    hallu_change = (with_cot_hallu - no_cot_hallu) * 100
    print(f"å‡†ç¡®ç‡å˜åŒ–ï¼š{acc_change:+.2f}%")
    print(f"åŠ æƒF1å˜åŒ–ï¼š{f1_change:+.4f}")
    print(f"å¹»è§‰ç‡å˜åŒ–ï¼š{hallu_change:+.2f}%")
    print("=" * 70)
    print("ğŸ‰ Qwen2-VLå®˜æ–¹æ ‡å‡†æµç¨‹è¯„ä¼°å®Œæˆï¼")